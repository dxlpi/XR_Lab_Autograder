{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \".\"\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGS 160 Auto-Grader Notebook for Architect Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"architect_chosen\": 5,\n",
    "    \"bio_750_words\": 10,\n",
    "    \"bio_structure\": 10,\n",
    "    \"bio_references\": 10,\n",
    "    \"10_buildings_with_images\": 15,\n",
    "    \"image_quality\": 10,\n",
    "    \"image_citations\": 10,\n",
    "    \"personal_bio_photo\": 5,\n",
    "    \"doc_and_slides\": 5,\n",
    "    \"image_relevance\": 10,\n",
    "    \"presentation_polish\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract text from PDF\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"üîç Extracting text from: {pdf_path}\")\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    print(\"‚úî Extracted text from PDF\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_images_from_pdf(pdf_path, min_width=200, save_folder=\"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_extracted_images\"):\n",
    "    import os\n",
    "    import fitz  # PyMuPDF\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Use a new folder for saving images\n",
    "    if save_folder is None:\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        save_folder = os.path.join(os.path.dirname(pdf_path), f\"{base_name}_images\")\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    image_data = []\n",
    "    for page_index in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc[page_index].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.width < min_width:\n",
    "                continue\n",
    "            img_path = os.path.join(save_folder, f\"page{page_index+1}_img{img_index+1}.png\")\n",
    "            if pix.n < 5:\n",
    "                pix.save(img_path)\n",
    "            else:\n",
    "                pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix1.save(img_path)\n",
    "            image_data.append(img_path)\n",
    "\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate biography structure & word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biography(text):\n",
    "    print(\" Evaluating biography: checking word count and required sections\")\n",
    "    result = {}\n",
    "    doc = nlp(text)\n",
    "    result[\"word_count\"] = len([token.text for token in doc if token.is_alpha])\n",
    "    required_sections = [\"who they are\", \"studied\", \"first building\", \"significance\", \"influence\"]\n",
    "    section_hits = sum([1 for section in required_sections if section.lower() in text.lower()])\n",
    "    result[\"structure_score\"] = int((section_hits / len(required_sections)) * rubric[\"bio_structure\"])\n",
    "    result[\"score\"] = rubric[\"bio_750_words\"] if result[\"word_count\"] >= 700 else int((result[\"word_count\"] / 750) * rubric[\"bio_750_words\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_bio_score(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(f\"üß† Sending biography text to ChatGPT for evaluation of {architect_name}\")\n",
    "\n",
    "    # Initial grading prompt\n",
    "    prompt = f\"\"\"\n",
    "You are grading a student's biography of the architect {architect_name}.\n",
    "Evaluate:\n",
    "- Who they are\n",
    "- What they‚Äôre famous for\n",
    "- Where they studied\n",
    "- Significance in architecture\n",
    "- Influence of buildings\n",
    "- Types of buildings\n",
    "- First building attributed\n",
    "Give a score out of 10 and a 1-paragraph feedback.\n",
    "\"\"\"\n",
    "\n",
    "    # First GPT evaluation\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    first_feedback = response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"üí¨ Initial GPT Feedback:\\n\", first_feedback)\n",
    "\n",
    "    # Reconsideration prompt\n",
    "    retry_prompt = \"\"\"\n",
    "Was this scoring too harsh? Re-evaluate the student‚Äôs biography with more weight on effort and alignment with the assignment instructions. \n",
    "Still provide a score out of 10 and a paragraph explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # GPT reconsideration\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": retry_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_feedback}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    reconsidered_feedback = second_response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"‚úÖ Reconsidered GPT Feedback:\\n\", reconsidered_feedback)\n",
    "\n",
    "    return reconsidered_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract references from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_text(text):\n",
    "    print(\"üîç Extracting references from text\")\n",
    "    lines = text.split(\"\\n\")\n",
    "    references = []\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\(\\d{4}\\)\", line) and any(x in line.lower() for x in [\"doi\", \"archdaily\", \"e-architect\", \"https://\"]):\n",
    "            references.append(line.strip())\n",
    "    return references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_references(ref_list):\n",
    "    print(\" Evaluating references\")\n",
    "    if not ref_list:\n",
    "        return {\"valid_references\": 0, \"score\": 0}\n",
    "\n",
    "    joined_refs = \"\\n\".join(ref_list)\n",
    "    prompt = f\"\"\"\n",
    "You are an academic writing assistant.\n",
    "Below is a list of references extracted from a student's architecture assignment:\n",
    "\n",
    "{joined_refs}\n",
    "\n",
    "Evaluate the overall quality of these references based on the following:\n",
    "- Are they properly formatted in APA style?\n",
    "- Are they from credible sources (e.g., books, peer-reviewed journals, respected architecture websites)?\n",
    "- Are there enough academic references (minimum of 5 is ideal)?\n",
    "\n",
    "Give a score out of 10 for reference quality, and provide a short justification.\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create([prompt])\n",
    "    print(\"üìö Openai Reference Evaluation:\\n\", response.text)\n",
    "    score_match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "    score = int(score_match.group(1)) if score_match else min(len(ref_list), rubric[\"bio_references\"])\n",
    "    return {\"valid_references\": len(ref_list), \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(image_data):\n",
    "    print(\"üîç Evaluating image resolution\")\n",
    "    high_res_count = 0\n",
    "\n",
    "    for img_path in image_data:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                if width >= 1000 and height >= 1000:  # arbitrary high-res threshold\n",
    "                    high_res_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    score = int((high_res_count / max(1, len(image_data))) * rubric[\"image_quality\"])\n",
    "    return {\"high_res_count\": high_res_count, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score image relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_relevance(image_data, architect_name, debug=False):\n",
    "    print(\"üîç Evaluating image relevance using Openai\")\n",
    "    relevance_scores = []\n",
    "    for img in image_data:\n",
    "        prompt = f\"\"\"\n",
    "You are evaluating whether this image is relevant to a project on the architect {architect_name}.\n",
    "1. Does this image depict a building by {architect_name}? If yes, say which building if you can.\n",
    "2. Is this an interior or exterior shot?\n",
    "3. Is this a high-quality academic image that clearly shows architectural features (composition, lighting, layout)?\n",
    "Give a score out of 10 for academic relevance with a brief justification.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = vision_openai.ChatCompletion.create([img[\"image\"], prompt])\n",
    "            if debug:\n",
    "                print(f\"üì∑ Openai Vision Feedback (Page {img['page']}):\", response.text)\n",
    "            match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "            score = int(match.group(1)) if match else 5\n",
    "        except:\n",
    "            score = 5\n",
    "        relevance_scores.append(score)\n",
    "    avg_score = sum(relevance_scores) / max(1, len(relevance_scores))\n",
    "    return {\"avg_score\": avg_score, \"score\": int((avg_score / 10) * rubric[\"image_relevance\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score remaining rubric items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using chain of thought "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scorecard(scores):\n",
    "    print(\" Generating scorecard\")\n",
    "    total = sum([v[\"score\"] for v in scores.values()])\n",
    "    return {\n",
    "        \"scorecard\": {k: v[\"score\"] for k, v in scores.items()},\n",
    "        \"final_score\": total,\n",
    "        \"grade\": \"A\" if total >= 90 else \"B\" if total >= 80 else \"C\" if total >= 70 else \"D\",\n",
    "        \"details\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autograder(pdf_path, architect_name):\n",
    "    print(\" Starting pipeline\")\n",
    "    doc_text = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    references = extract_references_from_text(doc_text)\n",
    "\n",
    "    scores = {\n",
    "        \"bio_750_words\": {\"score\": evaluate_biography(doc_text)[\"score\"]},\n",
    "        \"bio_structure\": {\"score\": evaluate_biography(doc_text)[\"structure_score\"]},\n",
    "        \"bio_references\": evaluate_references(references),\n",
    "        \"image_quality\": evaluate_image_quality(images),\n",
    "        \"image_relevance\": evaluate_image_relevance(images, architect_name)\n",
    "    }\n",
    "\n",
    "    rubric_feedback = openai_full_rubric_eval(doc_text, architect_name)\n",
    "    print(\"\\nüìã GPT Rubric Feedback:\\n\", rubric_feedback)\n",
    "\n",
    "    bio_feedback = openai_bio_score(doc_text, architect_name)\n",
    "    print(\"\\nüß† GPT Bio Score:\\n\", bio_feedback)\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting pipeline\n",
      "üîç Extracting text from: /Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "‚úî Extracted text from PDF\n",
      "üîç Extracting references from text\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating references\n",
      "üîç Evaluating image resolution\n",
      "üîç Evaluating image relevance using Openai\n",
      "\n",
      "üìã GPT Rubric Feedback:\n",
      " Here's a grading of the student's submission based on the provided rubric:\n",
      "\n",
      "* **1. Architect Selection & Scope (5/5):** The selection of Kazuyo Sejima is a great choice. With her distinct minimalist approach and key role in contemporary architecture, there is much to analyze and learn from her work. The biography is comprehensive and covers her transformative career, architectural philosophy, and major works.\n",
      "\n",
      "* **2. Organization & Document Setup (4.5/5):** The document is well-structured with clear headings and subheadings, throughout all sections. The table of contents makes navigating the document easier. Yet, certain aspects like the introduction and conclusion could be more clearly demarcated.\n",
      "\n",
      "* **3. Image Citation & Attribution (4.5/5):** Image citations and attributions are generally well managed, but they could follow a uniform formatting across. Several images lack proper captions which would otherwise provide insight into the significance of the image in context with Sejima's work.\n",
      "\n",
      "* **4. Coverage of 10 Buildings (5/5):** Excellent work here. The student covered more than ten buildings, each with a brief description, location, significance, and properly cited images. Buildings chosen show a range of Sejima's work, which bolsters the discussion in the biography.\n",
      "\n",
      "* **5. Student Bio & Photo (3/5):** While the student provides a succinct bio at the beginning, the photo, if any, is absent.\n",
      "\n",
      "* **6. Presentation Polish (4.5/5):** The overall presentation of the submission is visually polished, but could use some minor refining for consistency in image captioning and formatting.\n",
      "\n",
      "**Overall Comments:**\n",
      "This is an extremely well-researched and presented submission on Kazuyo Sejima. The biography is comprehensive and informative, and the coverage of her works is extensive and well-documented with images and appropriate image citations. The student bio is concise, but lacks a photograph. Minor improvements in layout formatting and image captioning could elevate the quality of presentation.\n",
      "\n",
      "üß† GPT Bio Score:\n",
      " The original text to assess wasn't provided, please provide the text for an accurate evaluation.\n",
      " Evaluation complete.\n",
      "{\n",
      "  \"bio_750_words\": {\n",
      "    \"score\": 10\n",
      "  },\n",
      "  \"bio_structure\": {\n",
      "    \"score\": 6\n",
      "  },\n",
      "  \"bio_references\": {\n",
      "    \"valid_references\": 0,\n",
      "    \"score\": 0\n",
      "  },\n",
      "  \"image_quality\": {\n",
      "    \"high_res_count\": 26,\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"image_relevance\": {\n",
      "    \"avg_score\": 5.0,\n",
      "    \"score\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = run_autograder(pdf_path, \"Kazuyo Sejima\")  \n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
