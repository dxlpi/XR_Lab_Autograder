{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGS 160 Auto-Grader Notebook for Architect Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"architect_chosen\": 5,\n",
    "    \"bio_750_words\": 10,\n",
    "    \"bio_structure\": 10,\n",
    "    \"bio_references\": 10,\n",
    "    \"10_buildings_with_images\": 15,\n",
    "    \"image_quality\": 10,\n",
    "    \"image_citations\": 10,\n",
    "    \"personal_bio_photo\": 5,\n",
    "    \"doc_and_slides\": 5,\n",
    "    \"image_relevance\": 10,\n",
    "    \"presentation_polish\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract text from PDF\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"üîç Extracting text from: {pdf_path}\")\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    print(\"‚úî Extracted text from PDF\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_images_from_pdf(pdf_path, min_width=200, save_folder=None):\n",
    "    import os\n",
    "    import fitz  # PyMuPDF\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Use a new folder for saving images\n",
    "    if save_folder is None:\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        save_folder = os.path.join(os.path.dirname(pdf_path), f\"{base_name}_images\")\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    image_data = []\n",
    "    for page_index in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc[page_index].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.width < min_width:\n",
    "                continue\n",
    "            img_path = os.path.join(save_folder, f\"page{page_index+1}_img{img_index+1}.png\")\n",
    "            if pix.n < 5:\n",
    "                pix.save(img_path)\n",
    "            else:\n",
    "                pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix1.save(img_path)\n",
    "            image_data.append(img_path)\n",
    "\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate biography structure & word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biography(text):\n",
    "    print(\" Evaluating biography: checking word count and required sections\")\n",
    "    result = {}\n",
    "    doc = nlp(text)\n",
    "    result[\"word_count\"] = len([token.text for token in doc if token.is_alpha])\n",
    "    required_sections = [\"who they are\", \"studied\", \"first building\", \"significance\", \"influence\"]\n",
    "    section_hits = sum([1 for section in required_sections if section.lower() in text.lower()])\n",
    "    result[\"structure_score\"] = int((section_hits / len(required_sections)) * rubric[\"bio_structure\"])\n",
    "    result[\"score\"] = rubric[\"bio_750_words\"] if result[\"word_count\"] >= 700 else int((result[\"word_count\"] / 750) * rubric[\"bio_750_words\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_bio_score(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(f\"üß† Sending biography text to ChatGPT for evaluation of {architect_name}\")\n",
    "\n",
    "    # Initial grading prompt\n",
    "    prompt = f\"\"\"\n",
    "You are grading a student's biography of the architect {architect_name}.\n",
    "Evaluate:\n",
    "- Who they are\n",
    "- What they‚Äôre famous for\n",
    "- Where they studied\n",
    "- Significance in architecture\n",
    "- Influence of buildings\n",
    "- Types of buildings\n",
    "- First building attributed\n",
    "Give a score out of 10 and a 1-paragraph feedback.\n",
    "\"\"\"\n",
    "\n",
    "    # First GPT evaluation\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    first_feedback = response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"üí¨ Initial GPT Feedback:\\n\", first_feedback)\n",
    "\n",
    "    # Reconsideration prompt\n",
    "    retry_prompt = \"\"\"\n",
    "Was this scoring too harsh? Re-evaluate the student‚Äôs biography with more weight on effort and alignment with the assignment instructions. \n",
    "Still provide a score out of 10 and a paragraph explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # GPT reconsideration\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": retry_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_feedback}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    reconsidered_feedback = second_response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"‚úÖ Reconsidered GPT Feedback:\\n\", reconsidered_feedback)\n",
    "\n",
    "    return reconsidered_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract references from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_text(text):\n",
    "    print(\"üîç Extracting references from text\")\n",
    "    lines = text.split(\"\\n\")\n",
    "    references = []\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\(\\d{4}\\)\", line) and any(x in line.lower() for x in [\"doi\", \"archdaily\", \"e-architect\", \"https://\"]):\n",
    "            references.append(line.strip())\n",
    "    return references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_references(ref_list):\n",
    "    print(\" Evaluating references\")\n",
    "    if not ref_list:\n",
    "        return {\"valid_references\": 0, \"score\": 0}\n",
    "\n",
    "    joined_refs = \"\\n\".join(ref_list)\n",
    "    prompt = f\"\"\"\n",
    "You are an academic writing assistant.\n",
    "Below is a list of references extracted from a student's architecture assignment:\n",
    "\n",
    "{joined_refs}\n",
    "\n",
    "Evaluate the overall quality of these references based on the following:\n",
    "- Are they properly formatted in APA style?\n",
    "- Are they from credible sources (e.g., books, peer-reviewed journals, respected architecture websites)?\n",
    "- Are there enough academic references (minimum of 5 is ideal)?\n",
    "\n",
    "Give a score out of 10 for reference quality, and provide a short justification.\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create([prompt])\n",
    "    print(\"üìö Openai Reference Evaluation:\\n\", response.text)\n",
    "    score_match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "    score = int(score_match.group(1)) if score_match else min(len(ref_list), rubric[\"bio_references\"])\n",
    "    return {\"valid_references\": len(ref_list), \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(image_data):\n",
    "    print(\"üîç Evaluating image resolution\")\n",
    "    high_res_count = 0\n",
    "\n",
    "    for img_path in image_data:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                if width >= 1000 and height >= 1000:  # arbitrary high-res threshold\n",
    "                    high_res_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    score = int((high_res_count / max(1, len(image_data))) * rubric[\"image_quality\"])\n",
    "    return {\"high_res_count\": high_res_count, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score image relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_relevance(image_data, architect_name, debug=False):\n",
    "    print(\"üîç Evaluating image relevance using Openai\")\n",
    "    relevance_scores = []\n",
    "    for img in image_data:\n",
    "        prompt = f\"\"\"\n",
    "You are evaluating whether this image is relevant to a project on the architect {architect_name}.\n",
    "1. Does this image depict a building by {architect_name}? If yes, say which building if you can.\n",
    "2. Is this an interior or exterior shot?\n",
    "3. Is this a high-quality academic image that clearly shows architectural features (composition, lighting, layout)?\n",
    "Give a score out of 10 for academic relevance with a brief justification.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = vision_openai.ChatCompletion.create([img[\"image\"], prompt])\n",
    "            if debug:\n",
    "                print(f\"üì∑ Openai Vision Feedback (Page {img['page']}):\", response.text)\n",
    "            match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "            score = int(match.group(1)) if match else 5\n",
    "        except:\n",
    "            score = 5\n",
    "        relevance_scores.append(score)\n",
    "    avg_score = sum(relevance_scores) / max(1, len(relevance_scores))\n",
    "    return {\"avg_score\": avg_score, \"score\": int((avg_score / 10) * rubric[\"image_relevance\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score remaining rubric items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using chain of thought "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scorecard(scores):\n",
    "    print(\" Generating scorecard\")\n",
    "    total = sum([v[\"score\"] for v in scores.values()])\n",
    "    return {\n",
    "        \"scorecard\": {k: v[\"score\"] for k, v in scores.items()},\n",
    "        \"final_score\": total,\n",
    "        \"grade\": \"A\" if total >= 90 else \"B\" if total >= 80 else \"C\" if total >= 70 else \"D\",\n",
    "        \"details\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autograder(pdf_path, architect_name):\n",
    "    print(\" Starting pipeline\")\n",
    "    doc_text = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    references = extract_references_from_text(doc_text)\n",
    "\n",
    "    scores = {\n",
    "        \"bio_750_words\": {\"score\": evaluate_biography(doc_text)[\"score\"]},\n",
    "        \"bio_structure\": {\"score\": evaluate_biography(doc_text)[\"structure_score\"]},\n",
    "        \"bio_references\": evaluate_references(references),\n",
    "        \"image_quality\": evaluate_image_quality(images),\n",
    "        \"image_relevance\": evaluate_image_relevance(images, architect_name)\n",
    "    }\n",
    "\n",
    "    rubric_feedback = openai_full_rubric_eval(doc_text, architect_name)\n",
    "    print(\"\\nüìã GPT Rubric Feedback:\\n\", rubric_feedback)\n",
    "\n",
    "    bio_feedback = openai_bio_score(doc_text, architect_name)\n",
    "    print(\"\\nüß† GPT Bio Score:\\n\", bio_feedback)\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting pipeline\n",
      "üîç Extracting text from: /Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "‚úî Extracted text from PDF\n",
      "üîç Extracting references from text\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating references\n",
      "üîç Evaluating image resolution\n",
      "üîç Evaluating image relevance using Openai\n",
      "\n",
      "üìã GPT Rubric Feedback:\n",
      " Here's a grading of the student's submission based on the provided rubric:\n",
      "\n",
      "* **1. Architect Selection & Scope (5/5):** Good choice of architect, with a clear focus on her notable works. The report covers the full range of Sejima's work from small residential models to large public projects. All aspects of her career are explored, including her partnership with Ryue Nishizawa and the establishment of SANAA.\n",
      "  \n",
      "* **2. Organization & Document Setup (5/5):** The report is very well-organized, with a rational structure that includes biography, portfolio, and detailed discussion of major works. The report also includes a ToC and clear headings, which is effective for navigation.\n",
      "  \n",
      "* **3. Image Citation & Attribution (4/5):** All images seem to be adequately cited with proper attribution. However, it's unclear for quite a few images who the original photographer or source is, as the student primarily cites the webpage rather than the original creator. \n",
      "\n",
      "* **4. Coverage of 10 Buildings (5/5):** The student has covered 10 buildings in depth with detailed discussion of their significance and context, as well as several images of each. The write-ups provide clear insight into the buildings' importance within Sejima's portfolio.\n",
      "  \n",
      "* **5. Student Bio & Photo (4/5):** The bio is suitable, but the document seems to lack an accompanying student photo. If this is an oversight, please correct it for future submissions.\n",
      "  \n",
      "* **6. Presentation Polish (5/5):** The document is professionally presented, with no glaring errors or issues. The layout is visually appealing and the document appears to be meticulously edited.\n",
      "\n",
      "**Overall Comments:**\n",
      "The student should be applauded for their exceptional work. The report is comprehensive, well-researched, and insightful, showing the student's deep understanding of Kazuyo Sejima's work and architectural philosophy. The student could improve their work by ensuring that all image citations clearly indicate the original source. Additionally, the student bio could be enhanced by the inclusion of a photo. Otherwise, this is an impressive piece of scholarship that reflects both the student's commitment and the quality of our instruction.\n",
      "\n",
      "üß† GPT Bio Score:\n",
      " Score: 7.5/10\n",
      "\n",
      "The student has done an overall good job in researching Kazuyo Sejima's life and work, along with a focus on her architectural philosophy. It is clear that a significant effort has been invested in gathering and presenting the information, alongside the main milestones of Sejima's career. The paper is mostly well-structured and sticks to the assignment instructions.\n",
      "\n",
      "However, a few areas require some improvement. For instance, the student could have done better in providing a more detailed analysis of Sejima's first major project - the Platform Houses. Also, while the student recognizes the impact of Sejima's work on the field of architecture, the explanation lacks depth and specificity to fully appreciate her contributions. A more exhaustive analysis of Sejima's style and philosophy could have given this biography a more comprehensive perspective. Some parts of the biography seem to simply state information rather than provide an analysis or interpretation. \n",
      "\n",
      "In conclusion, while there is some room for improvement, the student's effort and dedication to the task is commendable. The writing style is organized, and the biography generally aligns with the assignment instructions. With slight revisions to improve depth and specificity of content, this work can be a high-quality biography.\n",
      " Evaluation complete.\n",
      "{\n",
      "  \"bio_750_words\": {\n",
      "    \"score\": 10\n",
      "  },\n",
      "  \"bio_structure\": {\n",
      "    \"score\": 6\n",
      "  },\n",
      "  \"bio_references\": {\n",
      "    \"valid_references\": 0,\n",
      "    \"score\": 0\n",
      "  },\n",
      "  \"image_quality\": {\n",
      "    \"high_res_count\": 26,\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"image_relevance\": {\n",
      "    \"avg_score\": 5.0,\n",
      "    \"score\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = run_autograder(pdf_path, \"Kazuyo Sejima\")  \n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
