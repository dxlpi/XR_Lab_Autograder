{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGS 160 Auto-Grader Notebook for Architect Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"architect_chosen\": 5,\n",
    "    \"bio_750_words\": 10,\n",
    "    \"bio_structure\": 10,\n",
    "    \"bio_references\": 10,\n",
    "    \"10_buildings_with_images\": 15,\n",
    "    \"image_quality\": 10,\n",
    "    \"image_citations\": 10,\n",
    "    \"personal_bio_photo\": 5,\n",
    "    \"doc_and_slides\": 5,\n",
    "    \"image_relevance\": 10,\n",
    "    \"presentation_polish\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract text from PDF\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"üîç Extracting text from: {pdf_path}\")\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    print(\"‚úî Extracted text from PDF\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, min_width=200, save_folder=\"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_extracted_images\"):\n",
    "    import os\n",
    "    import fitz  # PyMuPDF\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Use a new folder for saving images\n",
    "    if save_folder is None:\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        save_folder = os.path.join(os.path.dirname(pdf_path), f\"{base_name}_images\")\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    image_data = []\n",
    "    for page_index in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc[page_index].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.width < min_width:\n",
    "                continue\n",
    "            img_path = os.path.join(save_folder, f\"page{page_index+1}_img{img_index+1}.png\")\n",
    "            if pix.n < 5:\n",
    "                pix.save(img_path)\n",
    "            else:\n",
    "                pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix1.save(img_path)\n",
    "            image_data.append(img_path)\n",
    "\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate biography structure & word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biography(text):\n",
    "    print(\" Evaluating biography: checking word count and required sections\")\n",
    "    result = {}\n",
    "    doc = nlp(text)\n",
    "    result[\"word_count\"] = len([token.text for token in doc if token.is_alpha])\n",
    "    required_sections = [\"who they are\", \"studied\", \"first building\", \"significance\", \"influence\"]\n",
    "    section_hits = sum([1 for section in required_sections if section.lower() in text.lower()])\n",
    "    result[\"structure_score\"] = int((section_hits / len(required_sections)) * rubric[\"bio_structure\"])\n",
    "    result[\"score\"] = rubric[\"bio_750_words\"] if result[\"word_count\"] >= 700 else int((result[\"word_count\"] / 750) * rubric[\"bio_750_words\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_bio_score(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(f\"üß† Sending biography text to ChatGPT for evaluation of {architect_name}\")\n",
    "\n",
    "    # Initial grading prompt\n",
    "    prompt = f\"\"\"\n",
    "You are grading a student's biography of the architect {architect_name}.\n",
    "Evaluate:\n",
    "- Who they are\n",
    "- What they‚Äôre famous for\n",
    "- Where they studied\n",
    "- Significance in architecture\n",
    "- Influence of buildings\n",
    "- Types of buildings\n",
    "- First building attributed\n",
    "Give a score out of 10 and a 1-paragraph feedback.\n",
    "\"\"\"\n",
    "\n",
    "    # First GPT evaluation\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    first_feedback = response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"üí¨ Initial GPT Feedback:\\n\", first_feedback)\n",
    "\n",
    "    # Reconsideration prompt\n",
    "    retry_prompt = \"\"\"\n",
    "Was this scoring too harsh? Re-evaluate the student‚Äôs biography with more weight on effort and alignment with the assignment instructions. \n",
    "Still provide a score out of 10 and a paragraph explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # GPT reconsideration\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": retry_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_feedback}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    reconsidered_feedback = second_response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"‚úÖ Reconsidered GPT Feedback:\\n\", reconsidered_feedback)\n",
    "\n",
    "    return reconsidered_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract references from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_text(text):\n",
    "    print(\"üîç Extracting references from text\")\n",
    "    lines = text.split(\"\\n\")\n",
    "    references = []\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\(\\d{4}\\)\", line) and any(x in line.lower() for x in [\"doi\", \"archdaily\", \"e-architect\", \"https://\"]):\n",
    "            references.append(line.strip())\n",
    "    return references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_references(ref_list):\n",
    "    print(\" Evaluating references\")\n",
    "    if not ref_list:\n",
    "        return {\"valid_references\": 0, \"score\": 0}\n",
    "\n",
    "    joined_refs = \"\\n\".join(ref_list)\n",
    "    prompt = f\"\"\"\n",
    "You are an academic writing assistant.\n",
    "Below is a list of references extracted from a student's architecture assignment:\n",
    "\n",
    "{joined_refs}\n",
    "\n",
    "Evaluate the overall quality of these references based on the following:\n",
    "- Are they properly formatted in APA style?\n",
    "- Are they from credible sources (e.g., books, peer-reviewed journals, respected architecture websites)?\n",
    "- Are there enough academic references (minimum of 5 is ideal)?\n",
    "\n",
    "Give a score out of 10 for reference quality, and provide a short justification.\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create([prompt])\n",
    "    print(\"üìö Openai Reference Evaluation:\\n\", response.text)\n",
    "    score_match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "    score = int(score_match.group(1)) if score_match else min(len(ref_list), rubric[\"bio_references\"])\n",
    "    return {\"valid_references\": len(ref_list), \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(image_data):\n",
    "    print(\"üîç Evaluating image resolution\")\n",
    "    high_res_count = 0\n",
    "\n",
    "    for img_path in image_data:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                if width >= 1000 and height >= 1000:  # arbitrary high-res threshold\n",
    "                    high_res_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    score = int((high_res_count / max(1, len(image_data))) * rubric[\"image_quality\"])\n",
    "    return {\"high_res_count\": high_res_count, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score image relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_relevance(image_data, architect_name, debug=False):\n",
    "    print(\"üîç Evaluating image relevance using Openai\")\n",
    "    relevance_scores = []\n",
    "    for img in image_data:\n",
    "        prompt = f\"\"\"\n",
    "You are evaluating whether this image is relevant to a project on the architect {architect_name}.\n",
    "1. Does this image depict a building by {architect_name}? If yes, say which building if you can.\n",
    "2. Is this an interior or exterior shot?\n",
    "3. Is this a high-quality academic image that clearly shows architectural features (composition, lighting, layout)?\n",
    "Give a score out of 10 for academic relevance with a brief justification.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = vision_openai.ChatCompletion.create([img[\"image\"], prompt])\n",
    "            if debug:\n",
    "                print(f\"üì∑ Openai Vision Feedback (Page {img['page']}):\", response.text)\n",
    "            match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "            score = int(match.group(1)) if match else 5\n",
    "        except:\n",
    "            score = 5\n",
    "        relevance_scores.append(score)\n",
    "    avg_score = sum(relevance_scores) / max(1, len(relevance_scores))\n",
    "    return {\"avg_score\": avg_score, \"score\": int((avg_score / 10) * rubric[\"image_relevance\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score remaining rubric items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using chain of thought "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "Do not praise the students‚Äô scholarly effort in the feedback as their answers are likely produced by AI\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing. \n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scorecard(scores):\n",
    "    print(\" Generating scorecard\")\n",
    "    total = sum([v[\"score\"] for v in scores.values()])\n",
    "    return {\n",
    "        \"scorecard\": {k: v[\"score\"] for k, v in scores.items()},\n",
    "        \"final_score\": total,\n",
    "        \"grade\": \"A\" if total >= 90 else \"B\" if total >= 80 else \"C\" if total >= 70 else \"D\",\n",
    "        \"details\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autograder(pdf_path, architect_name):\n",
    "    print(\" Starting pipeline\")\n",
    "    doc_text = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    references = extract_references_from_text(doc_text)\n",
    "\n",
    "    scores = {\n",
    "        \"bio_750_words\": {\"score\": evaluate_biography(doc_text)[\"score\"]},\n",
    "        \"bio_structure\": {\"score\": evaluate_biography(doc_text)[\"structure_score\"]},\n",
    "        \"bio_references\": evaluate_references(references),\n",
    "        \"image_quality\": evaluate_image_quality(images),\n",
    "        \"image_relevance\": evaluate_image_relevance(images, architect_name)\n",
    "    }\n",
    "\n",
    "    rubric_feedback = openai_full_rubric_eval(doc_text, architect_name)\n",
    "    print(\"\\nüìã GPT Rubric Feedback:\\n\", rubric_feedback)\n",
    "\n",
    "    bio_feedback = openai_bio_score(doc_text, architect_name)\n",
    "    print(\"\\nüß† GPT Bio Score:\\n\", bio_feedback)\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting pipeline\n",
      "üîç Extracting text from: /Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "‚úî Extracted text from PDF\n",
      "üîç Extracting references from text\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating references\n",
      "üîç Evaluating image resolution\n",
      "üîç Evaluating image relevance using Openai\n",
      "\n",
      "üìã GPT Rubric Feedback:\n",
      " Here's a grading of the student's submission based on the provided rubric:\n",
      "\n",
      "* **1. Architect Selection & Scope (5/5):** The project is a comprehensive exploration of Kazuyo Sejima, a highly influential architect from Japan. It establishes her significance, career, and contribution to contemporary architecture objectively and thoroughly.\n",
      "* **2. Organization & Document Setup (5/5):** The submission is clearly structured and well-organized with relevant subheadings. The inclusion of a thorough and concise biography, in-depth discussions of buildings, well-formatted citations, image captions, and student bio enhance the submission‚Äôs readability. \n",
      "* **3. Image Citation & Attribution (4/5):** The vast majority of images are properly cited and attributed. However, it's unclear if permission was obtained to reproduce some of the images gathered from third-party websites, as some URLs do not ideally represent proper citation and attribution.\n",
      "* **4. Coverage of 10 Buildings (5/5):** The student did an excellent job examining ten buildings designed by Sejima. Each building is discussed in detail, including location, significance, and key design elements, enhancing understanding of Sejima‚Äôs architectural style and philosophy. \n",
      "* **5. Student Bio & Photo (4/5):** The student bio is present and provides considerable insight into the student‚Äôs interests and aspirations. However, the photo is missing from this submission.\n",
      "* **6. Presentation Polish (5/5):** The document shows a high degree of polish, with consistent formatting, professional language, and an accessible, visually appealing layout.\n",
      "\n",
      "**Overall Comments:**\n",
      "The document presents a comprehensive and insightful analysis of Kazuyo Sejima, her career, and architectural impact. The selection, coverage, and discussion of buildings show a good understanding of her work, enhancing the submission‚Äôs depth and relevance. Proper citation has been almost completely observed, with a mild concern over picture sources. The conclusion would be improved if a student photo were included as stated in the rubric. Overall, the submission effectively outlines key aspects of Sejima‚Äôs career, philosophy, and contributions to architecture. The quality of research and the document‚Äôs polish reflect a high level of commitment to academic excellence.\n",
      "\n",
      "üß† GPT Bio Score:\n",
      " The student earns a 9.5 out of 10. This re-evaluation takes into consideration the amount of effort that was evidently put into researching Kazuyo Sejima's life and career for the biography, as well as the student's care in ensuring alignment with assignment instructions. The biography is well-structured with comprehensive details about Sejima, her background, and her accomplishments, focusing particularly on her pioneering role in architecture. It smartly highlights momentous points in her career, specifying significant architectural projects such as the Platform Houses. Where the paper lacks slightly is in the depth of discussion about the specific architectural structures credited to Sejima and her individual contributions to each. A deeper dive into her unique contributions to some of these projects would have been insightful. Nevertheless, the paper is commendable and largely meets the assignment criteria. The effort in research is visible and deserves praise. The high-quality references chosen were crucial in enhancing the overall quality of the paper.\n",
      " Evaluation complete.\n",
      "{\n",
      "  \"bio_750_words\": {\n",
      "    \"score\": 10\n",
      "  },\n",
      "  \"bio_structure\": {\n",
      "    \"score\": 6\n",
      "  },\n",
      "  \"bio_references\": {\n",
      "    \"valid_references\": 0,\n",
      "    \"score\": 0\n",
      "  },\n",
      "  \"image_quality\": {\n",
      "    \"high_res_count\": 26,\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"image_relevance\": {\n",
      "    \"avg_score\": 5.0,\n",
      "    \"score\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = run_autograder(pdf_path, \"Kazuyo Sejima\")  \n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
