{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \".\"\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COGS 160 Auto-Grader Notebook for Architect Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import spacy\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"architect_chosen\": 5,\n",
    "    \"bio_750_words\": 10,\n",
    "    \"bio_structure\": 10,\n",
    "    \"bio_references\": 10,\n",
    "    \"10_buildings_with_images\": 15,\n",
    "    \"image_quality\": 10,\n",
    "    \"image_citations\": 10,\n",
    "    \"personal_bio_photo\": 5,\n",
    "    \"doc_and_slides\": 5,\n",
    "    \"image_relevance\": 10,\n",
    "    \"presentation_polish\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract text from PDF\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"üîç Extracting text from: {pdf_path}\")\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    print(\"‚úî Extracted text from PDF\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extract images from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, min_width=200, save_folder=\"/Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_extracted_images\"):\n",
    "    import os\n",
    "    import fitz  # PyMuPDF\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Use a new folder for saving images\n",
    "    if save_folder is None:\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        save_folder = os.path.join(os.path.dirname(pdf_path), f\"{base_name}_images\")\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    image_data = []\n",
    "    for page_index in range(len(doc)):\n",
    "        for img_index, img in enumerate(doc[page_index].get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.width < min_width:\n",
    "                continue\n",
    "            img_path = os.path.join(save_folder, f\"page{page_index+1}_img{img_index+1}.png\")\n",
    "            if pix.n < 5:\n",
    "                pix.save(img_path)\n",
    "            else:\n",
    "                pix1 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix1.save(img_path)\n",
    "            image_data.append(img_path)\n",
    "\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate biography structure & word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_biography(text):\n",
    "    print(\" Evaluating biography: checking word count and required sections\")\n",
    "    result = {}\n",
    "    doc = nlp(text)\n",
    "    result[\"word_count\"] = len([token.text for token in doc if token.is_alpha])\n",
    "    required_sections = [\"who they are\", \"studied\", \"first building\", \"significance\", \"influence\"]\n",
    "    section_hits = sum([1 for section in required_sections if section.lower() in text.lower()])\n",
    "    result[\"structure_score\"] = int((section_hits / len(required_sections)) * rubric[\"bio_structure\"])\n",
    "    result[\"score\"] = rubric[\"bio_750_words\"] if result[\"word_count\"] >= 700 else int((result[\"word_count\"] / 750) * rubric[\"bio_750_words\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_bio_score(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(f\"üß† Sending biography text to ChatGPT for evaluation of {architect_name}\")\n",
    "\n",
    "    # Initial grading prompt\n",
    "    prompt = f\"\"\"\n",
    "You are grading a student's biography of the architect {architect_name}.\n",
    "Evaluate:\n",
    "- Who they are\n",
    "- What they‚Äôre famous for\n",
    "- Where they studied\n",
    "- Significance in architecture\n",
    "- Influence of buildings\n",
    "- Types of buildings\n",
    "- First building attributed\n",
    "Give a score out of 10 and a 1-paragraph feedback.\n",
    "\"\"\"\n",
    "\n",
    "    # First GPT evaluation\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    first_feedback = response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"üí¨ Initial GPT Feedback:\\n\", first_feedback)\n",
    "\n",
    "    # Reconsideration prompt\n",
    "    retry_prompt = \"\"\"\n",
    "Was this scoring too harsh? Re-evaluate the student‚Äôs biography with more weight on effort and alignment with the assignment instructions. \n",
    "Still provide a score out of 10 and a paragraph explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # GPT reconsideration\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": retry_prompt},\n",
    "            {\"role\": \"user\", \"content\": first_feedback}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    reconsidered_feedback = second_response.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"‚úÖ Reconsidered GPT Feedback:\\n\", reconsidered_feedback)\n",
    "\n",
    "    return reconsidered_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract references from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_text(text):\n",
    "    print(\"üîç Extracting references from text\")\n",
    "    lines = text.split(\"\\n\")\n",
    "    references = []\n",
    "    for line in lines:\n",
    "        if re.search(r\"\\(\\d{4}\\)\", line) and any(x in line.lower() for x in [\"doi\", \"archdaily\", \"e-architect\", \"https://\"]):\n",
    "            references.append(line.strip())\n",
    "    return references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_references(ref_list):\n",
    "    print(\" Evaluating references\")\n",
    "    if not ref_list:\n",
    "        return {\"valid_references\": 0, \"score\": 0}\n",
    "\n",
    "    joined_refs = \"\\n\".join(ref_list)\n",
    "    prompt = f\"\"\"\n",
    "You are an academic writing assistant.\n",
    "Below is a list of references extracted from a student's architecture assignment:\n",
    "\n",
    "{joined_refs}\n",
    "\n",
    "Evaluate the overall quality of these references based on the following:\n",
    "- Are they properly formatted in APA style?\n",
    "- Are they from credible sources (e.g., books, peer-reviewed journals, respected architecture websites)?\n",
    "- Are there enough academic references (minimum of 5 is ideal)?\n",
    "\n",
    "Give a score out of 10 for reference quality, and provide a short justification.\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create([prompt])\n",
    "    print(\"üìö Openai Reference Evaluation:\\n\", response.text)\n",
    "    score_match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "    score = int(score_match.group(1)) if score_match else min(len(ref_list), rubric[\"bio_references\"])\n",
    "    return {\"valid_references\": len(ref_list), \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(image_data):\n",
    "    print(\"üîç Evaluating image resolution\")\n",
    "    high_res_count = 0\n",
    "\n",
    "    for img_path in image_data:\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                width, height = img.size\n",
    "                if width >= 1000 and height >= 1000:  # arbitrary high-res threshold\n",
    "                    high_res_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    score = int((high_res_count / max(1, len(image_data))) * rubric[\"image_quality\"])\n",
    "    return {\"high_res_count\": high_res_count, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score image relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_relevance(image_data, architect_name, debug=False):\n",
    "    print(\"üîç Evaluating image relevance using Openai\")\n",
    "    relevance_scores = []\n",
    "    for img in image_data:\n",
    "        prompt = f\"\"\"\n",
    "You are evaluating whether this image is relevant to a project on the architect {architect_name}.\n",
    "1. Does this image depict a building by {architect_name}? If yes, say which building if you can.\n",
    "2. Is this an interior or exterior shot?\n",
    "3. Is this a high-quality academic image that clearly shows architectural features (composition, lighting, layout)?\n",
    "Give a score out of 10 for academic relevance with a brief justification.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = vision_openai.ChatCompletion.create([img[\"image\"], prompt])\n",
    "            if debug:\n",
    "                print(f\"üì∑ Openai Vision Feedback (Page {img['page']}):\", response.text)\n",
    "            match = re.search(r\"(\\d{1,2})/10\", response.text)\n",
    "            score = int(match.group(1)) if match else 5\n",
    "        except:\n",
    "            score = 5\n",
    "        relevance_scores.append(score)\n",
    "    avg_score = sum(relevance_scores) / max(1, len(relevance_scores))\n",
    "    return {\"avg_score\": avg_score, \"score\": int((avg_score / 10) * rubric[\"image_relevance\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Openai: score remaining rubric items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing.\n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using chain of thought "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def openai_full_rubric_eval(text, architect_name, debug=False):\n",
    "    if debug:\n",
    "        print(\"üß† ChatGPT evaluating extended rubric\")\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are an architecture professor grading a student's final submission about {architect_name}.\n",
    "The submission includes:\n",
    "- A 750-word biography of the architect\n",
    "- A discussion and visual documentation of at least 10 buildings\n",
    "- MLA-formatted citations\n",
    "- Image captions with proper attribution\n",
    "- A student bio and photo\n",
    "- A visually polished and clearly structured document\n",
    "\n",
    "Grade the submission across the following rubric categories, each out of 5:\n",
    "1. Architect Selection & Scope\n",
    "2. Organization & Document Setup\n",
    "3. Image Citation & Attribution\n",
    "4. Coverage of 10 Buildings\n",
    "5. Student Bio & Photo\n",
    "6. Presentation Polish\n",
    "\n",
    "You must include and score **all 6 rubric categories** regardless of whether the content is present. \n",
    "If a section is missing or inadequate, explain briefly and assign a lower score accordingly.\n",
    "Do not praise the students‚Äô scholarly effort in the feedback as their answers are likely produced by AI\n",
    "\n",
    "Return your response in this format:\n",
    "```\n",
    "Here's a grading of the student's submission based on the provided rubric:\n",
    "\n",
    "* **1. Architect Selection & Scope (X/5):** ...\n",
    "* **2. Organization & Doc Setup (X/5):** ...\n",
    "* **3. Image Citation & Attribution (X/5):** ...\n",
    "* **4. Coverage of 10 Buildings (X/5):** ...\n",
    "* **5. Student Bio & Photo (X/5):** ...\n",
    "* **6. Presentation Polish (X/5):** ...\n",
    "\n",
    "**Overall Comments:**\n",
    "<paragraph summarizing the work>\n",
    "```\n",
    "Only provide the formatted text shown above. Do not omit any of the six rubric categories, even if evidence is missing. \n",
    "\"\"\"\n",
    "\n",
    "    user_content = text\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scorecard(scores):\n",
    "    print(\" Generating scorecard\")\n",
    "    total = sum([v[\"score\"] for v in scores.values()])\n",
    "    return {\n",
    "        \"scorecard\": {k: v[\"score\"] for k, v in scores.items()},\n",
    "        \"final_score\": total,\n",
    "        \"grade\": \"A\" if total >= 90 else \"B\" if total >= 80 else \"C\" if total >= 70 else \"D\",\n",
    "        \"details\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autograder(pdf_path, architect_name):\n",
    "    print(\" Starting pipeline\")\n",
    "    doc_text = extract_text_from_pdf(pdf_path)\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    references = extract_references_from_text(doc_text)\n",
    "\n",
    "    scores = {\n",
    "        \"bio_750_words\": {\"score\": evaluate_biography(doc_text)[\"score\"]},\n",
    "        \"bio_structure\": {\"score\": evaluate_biography(doc_text)[\"structure_score\"]},\n",
    "        \"bio_references\": evaluate_references(references),\n",
    "        \"image_quality\": evaluate_image_quality(images),\n",
    "        \"image_relevance\": evaluate_image_relevance(images, architect_name)\n",
    "    }\n",
    "\n",
    "    rubric_feedback = openai_full_rubric_eval(doc_text, architect_name)\n",
    "    print(\"\\nüìã GPT Rubric Feedback:\\n\", rubric_feedback)\n",
    "\n",
    "    bio_feedback = openai_bio_score(doc_text, architect_name)\n",
    "    print(\"\\nüß† GPT Bio Score:\\n\", bio_feedback)\n",
    "\n",
    "    print(\" Evaluation complete.\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting pipeline\n",
      "üîç Extracting text from: /Users/heather/Desktop/Work/XR Lab/A1 Submissions/davidmatthew_LATE_134808_14949557_COGS 160_ A1.pdf\n",
      "‚úî Extracted text from PDF\n",
      "üîç Extracting references from text\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating biography: checking word count and required sections\n",
      " Evaluating references\n",
      "üîç Evaluating image resolution\n",
      "üîç Evaluating image relevance using Openai\n",
      "\n",
      "üìã GPT Rubric Feedback:\n",
      " Here's a grading of the student's submission based on the provided rubric:\n",
      "\n",
      "* **1. Architect Selection & Scope (5/5):** The student's choice of Kazuyo Sejima as the architect is commendable. She is indeed an influential figure in contemporary architecture. The scope of the work appropriately covers biographical data, architectural philosophy, career progression, and contribution to the profession. \n",
      "\n",
      "* **2. Organization & Document Setup (5/5):** The organization of the document is excellent and easy to follow. It begins with an introduction of the student and the architect, followed by a comprehensive bio, a detailed study of ten buildings, including image references, and wraps up with appropriately formatted MLA citations.\n",
      "\n",
      "* **3. Image Citation & Attribution (3/5):** While the document does include image citations for every building mentioned, the attribution of these images seems inconsistent and incomplete in some areas. The URLs are provided, but the student needs to include the source's name, and in some cases, the author or photographer.\n",
      "\n",
      "* **4. Coverage of 10 Buildings (5/5):** The student has taken a thorough approach in discussing the architect‚Äôs work, offering critical analysis and visual material for ten different buildings designed by Sejima. Each building discussion incorporates the location, significance, and related images, providing comprehensive coverage.\n",
      "\n",
      "* **5. Student Bio & Photo (4/5):** The submission includes an informative bio of the student, detailing their undergraduate status, area of study, and interest in architecture. However, the student's photo, which was a requirement, is missing.\n",
      "\n",
      "* **6. Presentation Polish (4/5):** The document appears polished and professional, with a clear, coherent flow of content. However, the formatting could be improved for readability, such as creating more distinct divisions between each building discussion. The visuals could also be embedded more smoothly into the document structure.\n",
      "\n",
      "**Overall Comments:**\n",
      "This submission is thorough and well-researched, offering a comprehensive study of architect Kazuyo Sejima. The document has been structured logically, beginning with a student introduction and architect bio, moving into coverage of ten selected buildings, and concluding with detailed MLA references. However, the credibility of the work could be enhanced by ensuring complete image citations and attributions. Additionally, including the required student photo and paying more attention to document formatting might enhance the presentation's overall quality.\n",
      "\n",
      "üß† GPT Bio Score:\n",
      " The student biography was almost perfect. It scored 9/10. \n",
      "\n",
      "The biography is well organized, in-depth, and adheres to the assignment instructions. It clearly reflects the significant amount of time and effort invested into researching Kazuyo Sejima's life and career. The narrative is compelling, and the student adeptly balances facts with interpretation, resulting in an engaging and informative biography.\n",
      "\n",
      "Looking at areas to improve, the biography could have been clearer in distinguishing between Kazuyo Sejima's solo projects and those done under SANAA. Furthermore, while the student mentioned a few of her most famous works, it would have been beneficial to offer more detailed exploration of her lesser-known projects as well. Given this minor shortcoming, the assignment does not merit the full 10 points.\n",
      "\n",
      "In summary, the biography is nearly perfect but for a more holistic view of Sejima's career, it could benefit from an expanded discussion of her wider body of work.\n",
      " Evaluation complete.\n",
      "{\n",
      "  \"bio_750_words\": {\n",
      "    \"score\": 10\n",
      "  },\n",
      "  \"bio_structure\": {\n",
      "    \"score\": 6\n",
      "  },\n",
      "  \"bio_references\": {\n",
      "    \"valid_references\": 0,\n",
      "    \"score\": 0\n",
      "  },\n",
      "  \"image_quality\": {\n",
      "    \"high_res_count\": 26,\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"image_relevance\": {\n",
      "    \"avg_score\": 5.0,\n",
      "    \"score\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = run_autograder(pdf_path, \"Kazuyo Sejima\")  \n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
